---
slug: onnx-on-web
title: Onnx web runtime êµ¬ì¶•í•˜ê¸° with Vite
authors: mooyeon
tags: [web, onnx, vit]
date: 2025-07-28T20:25
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Onnx web runtime êµ¬ì¶•í•˜ê¸° with Vite

## ì†Œê°œ

Vission AIë¥¼ ì‚¬ìš©ì ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ WebGPU, WebGL, WASMì„ í†µí•´ Webí™˜ê²½ì—ì„œ AI Modelì„ ì‹¤í–‰í•œ ë°©ë²•ë“¤ê³¼ Modelì„ ìºì‹±í•˜ë©° ë²„ì „ê´€ë¦¬ë¥¼ í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì‘ì„±í•˜ì˜€ë‹¤.

:::info ëª©ì°¨

1. [ë°°ê²½](#ë°°ê²½)
2. [Get Started](#get-started)
3. [ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ì‹¤í–‰](#ëª¨ë¸-ë¡œë“œ-ë°-ì¶”ë¡ -ì‹¤í–‰)
4. [ë°±ì—”ë“œ ì„ íƒ ê°€ì´ë“œ](#ë°±ì—”ë“œ-ì„ íƒ-ê°€ì´ë“œ)
5. [ëª¨ë¸ ìºì‹± ë° ë²„ì „ ê´€ë¦¬](#ëª¨ë¸-ìºì‹±-ë°-ë²„ì „-ê´€ë¦¬)
6. [ì‹¤ì œ Vision AI ëª¨ë¸ ì ìš© ì˜ˆì œ](#ì‹¤ì œ-vision-ai-ëª¨ë¸-ì ìš©-ì˜ˆì œ)
7. [ì„±ëŠ¥ ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬](#ì„±ëŠ¥-ìµœì í™”-ë°-ë©”ëª¨ë¦¬-ê´€ë¦¬)
8. [ì—ëŸ¬ ì²˜ë¦¬ ë° ë””ë²„ê¹…](#ì—ëŸ¬-ì²˜ë¦¬-ë°-ë””ë²„ê¹…)
9. [ë§ˆë¬´ë¦¬](#ë§ˆë¬´ë¦¬)

:::

## ë°°ê²½

## Get Started

ONNX Runtime Webì„ ì ìš©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ [ê³µì‹ì‚¬ì´íŠ¸](https://onnxruntime.ai/)ì—ì„œëŠ” ì•„ë˜ì˜ ë°©ë²•ë“¤ì„ ì†Œê°œí•œë‹¤.

### JavaScript import statement

#### Install

<Tabs>
  <TabItem value="npm" label="npm" default>
    ```bash
    # install latest release version
    npm install onnxruntime-web

    # install nightly build dev version
    npm install onnxruntime-web@dev
    ```

  </TabItem>
  <TabItem value="yarn" label="yarn">
    ```bash
    # install latest release version
    yarn add onnxruntime-web

    # install nightly build dev version
    yarn add onnxruntime-web@dev
    ```

  </TabItem>
</Tabs>

#### Import

```js
import * as ort from "onnxruntime-web";
```

í•˜ì§€ë§Œ ìœ„ ë°©ë²•ëŒ€ë¡œë§Œ ì ìš©í•œë‹¤ë©´

```bash
no available backend found. ERR: [webgpu] RuntimeError: Aborted(both async and sync fetching of the wasm failed). Build with -sASSERTIONS for more info., [webgl] backend not found., [wasm] Error: previous call to 'initWasm()' failed.
```

ì™€ ê°™ì´ `no available backend found` ì—ëŸ¬ê°€ ë°œìƒí•œë‹¤. í˜„ì¬ í”„ë¡œì íŠ¸ í™˜ê²½ì€ `React Router V7 + Vite` ì´ë¯€ë¡œ ì´ì— ì¶”ê°€ì ì¸ ì„¤ì •ì´ í•„ìš”í•˜ë‹¤

```ts
...
export default defineConfig({
  ...
  ...
  assetsInclude: ["**/*.onnx"],
  optimizeDeps: {
    exclude: ["onnxruntime-web"],
  },
});
```

- `assetsInclude: ["**/*.onnx"]`

  - `.onnx` íŒŒì¼ì„ ì •ì  ìì‚°(asset)ìœ¼ë¡œ ì²˜ë¦¬
  - í•´ë‹¹ íŒŒì¼ë“¤ì€ ë¹Œë“œ ì‹œ `dist/` í´ë”ë¡œ ë³µì‚¬ë˜ê³  URLì„ í†µí•´ ì ‘ê·¼ ê°€ëŠ¥
  - ëª¨ë¸ íŒŒì¼ì„ `import` í•˜ê±°ë‚˜ `public` í´ë”ì—ì„œ `fetch` í•  ìˆ˜ ìˆë„ë¡ í•´ì¤Œ

- `optimizeDeps: exclude: ["onnxruntime-web"]`
  - `onnxruntime-web` íŒ¨í‚¤ì§€ë¥¼ Vite ì‚¬ì „ ë²ˆë“¤ë§(pre-bundling)ì—ì„œ ì œì™¸
  - ì´ íŒ¨í‚¤ì§€ëŠ” WASM íŒŒì¼ê³¼ ë³µì¡í•œ ë¡œë”© ë¡œì§ì„ í¬í•¨í•˜ë¯€ë¡œ ëŸ°íƒ€ì„ì— ë™ì ìœ¼ë¡œ ë¡œë“œë˜ì–´ì•¼ í•¨
  - Viteê°€ ì´ íŒ¨í‚¤ì§€ë¥¼ ESMìœ¼ë¡œ ë³€í™˜í•˜ë ¤ë‹¤ ì‹¤íŒ¨í•˜ëŠ” ê²ƒì„ ë°©ì§€

ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ë¼ë©´ CDN ì—ì„œ script ë¥¼ ë°›ì•„ì™€ `js`ë¡œ ì‹¤í–‰í•´ì£¼ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤.

### HTML script tag

#### Import

```html
<script src="https://example.com/path/ort.webgpu.min.js"></script>
```

#### ì‚¬ìš©ë²•

ì´ ê²½ìš° `window.`ì˜ `ort`ë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•˜ì—¬ì•¼ í•œë‹¤.

##### ì˜ˆì‹œ

```ts
const session = await window.ort.InferenceSession.create(modelSource as any, {
  executionProviders: ["webgpu", "webgl", "wasm"],
});
```

í•˜ì§€ë§Œ íƒ€ì… ì•ˆì „ì„±, ë²ˆë“¤ ìµœì í™”, ì˜¤í”„ë¼ì¸ ì§€ì›, ì˜ì¡´ì„± ê´€ë¦¬, ë³´ì•ˆ ë“±ì˜ ë¬¸ì œë¡œ ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” `Package import` ë°©ì‹ì„ ì ìš©í•˜ë„ë¡ í•œë‹¤.

## ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ì‹¤í–‰

### ê¸°ë³¸ ì‚¬ìš©ë²•

ONNX Runtime Webì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì¶”ë¡ ì„ ì‹¤í–‰í•˜ëŠ” ê¸°ë³¸ì ì¸ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

```ts
import * as ort from "onnxruntime-web";

// ëª¨ë¸ ë¡œë“œ
async function loadModel(modelUrl: string) {
  try {
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ["webgpu", "webgl", "wasm"],
    });
    return session;
  } catch (error) {
    console.error("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:", error);
    throw error;
  }
}

// ì¶”ë¡  ì‹¤í–‰
async function runInference(session: ort.InferenceSession, inputData: Float32Array, inputShape: number[]) {
  try {
    // ì…ë ¥ í…ì„œ ìƒì„±
    const inputTensor = new ort.Tensor("float32", inputData, inputShape);
    
    // ì¶”ë¡  ì‹¤í–‰
    const feeds = { input: inputTensor }; // ëª¨ë¸ì˜ ì…ë ¥ ì´ë¦„ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
    const results = await session.run(feeds);
    
    return results;
  } catch (error) {
    console.error("ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨:", error);
    throw error;
  }
}
```

### ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì˜ˆì œ

Vision AI ëª¨ë¸ì˜ ê²½ìš° ì´ë¯¸ì§€ ì „ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤:

```ts
// ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬
function preprocessImage(imageElement: HTMLImageElement, targetSize: [number, number] = [224, 224]): Float32Array {
  const canvas = document.createElement("canvas");
  const ctx = canvas.getContext("2d")!;
  
  canvas.width = targetSize[0];
  canvas.height = targetSize[1];
  
  // ì´ë¯¸ì§€ë¥¼ ìº”ë²„ìŠ¤ì— ê·¸ë¦¬ê¸° (ë¦¬ì‚¬ì´ì¦ˆ)
  ctx.drawImage(imageElement, 0, 0, targetSize[0], targetSize[1]);
  
  // í”½ì…€ ë°ì´í„° ì¶”ì¶œ
  const imageData = ctx.getImageData(0, 0, targetSize[0], targetSize[1]);
  const pixels = imageData.data;
  
  // RGB ì •ê·œí™” ë° ì±„ë„ ë¶„ë¦¬ (CHW í˜•ì‹)
  const float32Data = new Float32Array(3 * targetSize[0] * targetSize[1]);
  
  for (let i = 0; i < pixels.length; i += 4) {
    const pixelIndex = i / 4;
    const r = pixels[i] / 255.0;
    const g = pixels[i + 1] / 255.0;
    const b = pixels[i + 2] / 255.0;
    
    // CHW í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë°°ì¹˜
    float32Data[pixelIndex] = r; // R ì±„ë„
    float32Data[pixelIndex + (targetSize[0] * targetSize[1])] = g; // G ì±„ë„
    float32Data[pixelIndex + (2 * targetSize[0] * targetSize[1])] = b; // B ì±„ë„
  }
  
  return float32Data;
}
```

### ì™„ì „í•œ ì‚¬ìš© ì˜ˆì œ

```ts
async function classifyImage(imageElement: HTMLImageElement) {
  try {
    // 1. ëª¨ë¸ ë¡œë“œ
    const session = await loadModel("/models/resnet50.onnx");
    
    // 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    const inputData = preprocessImage(imageElement);
    const inputShape = [1, 3, 224, 224]; // [batch, channels, height, width]
    
    // 3. ì¶”ë¡  ì‹¤í–‰
    const results = await runInference(session, inputData, inputShape);
    
    // 4. ê²°ê³¼ ì²˜ë¦¬
    const outputTensor = results[Object.keys(results)[0]]; // ì²« ë²ˆì§¸ ì¶œë ¥
    const predictions = outputTensor.data as Float32Array;
    
    // ìµœëŒ€ê°’ì˜ ì¸ë±ìŠ¤ ì°¾ê¸° (ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤)
    const maxIndex = predictions.indexOf(Math.max(...predictions));
    
    return {
      classIndex: maxIndex,
      confidence: predictions[maxIndex],
      allPredictions: predictions
    };
  } catch (error) {
    console.error("ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹¤íŒ¨:", error);
    throw error;
  }
}
```

## ë°±ì—”ë“œ ì„ íƒ ê°€ì´ë“œ

ONNX Runtime Webì€ 3ê°€ì§€ ë°±ì—”ë“œë¥¼ ì§€ì›í•˜ë©°, ê°ê°ì˜ íŠ¹ì„±ê³¼ ì„±ëŠ¥ì´ ë‹¤ë¥´ë‹¤:

### WebGPU ë°±ì—”ë“œ

**ì¥ì :**
- ìµœê³  ì„±ëŠ¥ (GPU ê°€ì†)
- ë³‘ë ¬ ì²˜ë¦¬ì— ìµœì í™”
- ëŒ€ìš©ëŸ‰ ëª¨ë¸ì— ì í•©

**ë‹¨ì :**
- ë¸Œë¼ìš°ì € ì§€ì› ì œí•œì  (Chrome 113+, Edge 113+)
- ê°œë°œ ë‹¨ê³„ì˜ ê¸°ìˆ 

**ì‚¬ìš© ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤:**
- ìµœì‹  ë¸Œë¼ìš°ì € í™˜ê²½
- ë³µì¡í•œ ëª¨ë¸ ë˜ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ê²½ìš°

```ts
const session = await ort.InferenceSession.create(modelUrl, {
  executionProviders: ["webgpu"],
});
```

### WebGL ë°±ì—”ë“œ

**ì¥ì :**
- ì¢‹ì€ ë¸Œë¼ìš°ì € í˜¸í™˜ì„±
- GPU ê°€ì† ì§€ì›
- WebGPUë³´ë‹¤ ì•ˆì •ì 

**ë‹¨ì :**
- WebGPU ëŒ€ë¹„ ì„±ëŠ¥ ì œí•œ
- ì¼ë¶€ ì—°ì‚°ì ì§€ì› ì œí•œ

**ì‚¬ìš© ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤:**
- ê´‘ë²”ìœ„í•œ ë¸Œë¼ìš°ì € í˜¸í™˜ì„±ì´ í•„ìš”í•œ ê²½ìš°
- ì¤‘ê°„ ê·œëª¨ì˜ ëª¨ë¸
- ì•ˆì •ì„±ì´ ì¤‘ìš”í•œ í”„ë¡œë•ì…˜ í™˜ê²½

```ts
const session = await ort.InferenceSession.create(modelUrl, {
  executionProviders: ["webgl"],
});
```

### WASM ë°±ì—”ë“œ

**ì¥ì :**
- ëª¨ë“  ë¸Œë¼ìš°ì € ì§€ì›
- ê°€ì¥ ì•ˆì •ì 
- CPU ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥

**ë‹¨ì :**
- ê°€ì¥ ëŠë¦° ì„±ëŠ¥
- GPU ê°€ì† ì—†ìŒ

**ì‚¬ìš© ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤:**
- ìµœëŒ€ í˜¸í™˜ì„±ì´ í•„ìš”í•œ ê²½ìš°
- ê°„ë‹¨í•œ ëª¨ë¸
- ë°±ì—… ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš©

```ts
const session = await ort.InferenceSession.create(modelUrl, {
  executionProviders: ["wasm"],
});
```

### ì„±ëŠ¥ ë¹„êµ

| ë°±ì—”ë“œ | ë¸Œë¼ìš°ì € ì§€ì› | ì„±ëŠ¥ | ì•ˆì •ì„± | ê¶Œì¥ ìš©ë„ |
|--------|---------------|------|--------|-----------|
| WebGPU | Chrome 113+, Edge 113+ | â­â­â­â­â­ | â­â­â­ | ìµœì‹  í™˜ê²½, ê³ ì„±ëŠ¥ |
| WebGL | ëŒ€ë¶€ë¶„ì˜ ëª¨ë˜ ë¸Œë¼ìš°ì € | â­â­â­â­ | â­â­â­â­ | ê· í˜• ì¡íŒ ì„ íƒ |
| WASM | ëª¨ë“  ë¸Œë¼ìš°ì € | â­â­ | â­â­â­â­â­ | í˜¸í™˜ì„± ìš°ì„  |

### ì ì‘í˜• ë°±ì—”ë“œ ì„ íƒ

ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë¸Œë¼ìš°ì € ì§€ì› ì—¬ë¶€ì— ë”°ë¼ ë°±ì—”ë“œë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤:

```ts
async function createSessionWithFallback(modelUrl: string) {
  const providers = ["webgpu", "webgl", "wasm"];
  
  for (const provider of providers) {
    try {
      console.log(`${provider} ë°±ì—”ë“œë¡œ ì‹œë„ ì¤‘...`);
      const session = await ort.InferenceSession.create(modelUrl, {
        executionProviders: [provider],
      });
      console.log(`${provider} ë°±ì—”ë“œë¡œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë¨`);
      return { session, provider };
    } catch (error) {
      console.warn(`${provider} ë°±ì—”ë“œ ì‹¤íŒ¨:`, error);
      continue;
    }
  }
  
  throw new Error("ëª¨ë“  ë°±ì—”ë“œì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨");
}
```

## ëª¨ë¸ ìºì‹± ë° ë²„ì „ ê´€ë¦¬

ì›¹ í™˜ê²½ì—ì„œ ONNX ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ìºì‹±ê³¼ ë²„ì „ ê´€ë¦¬ê°€ ì¤‘ìš”í•˜ë‹¤.

### IndexedDBë¥¼ ì´ìš©í•œ ëª¨ë¸ ìºì‹±

```ts
interface ModelCache {
  url: string;
  version: string;
  data: ArrayBuffer;
  timestamp: number;
}

class ONNXModelManager {
  private dbName = "onnx-models";
  private storeName = "models";
  private db: IDBDatabase | null = null;

  async initDB(): Promise<void> {
    return new Promise((resolve, reject) => {
      const request = indexedDB.open(this.dbName, 1);
      
      request.onerror = () => reject(request.error);
      request.onsuccess = () => {
        this.db = request.result;
        resolve();
      };
      
      request.onupgradeneeded = (event) => {
        const db = (event.target as IDBOpenDBRequest).result;
        if (!db.objectStoreNames.contains(this.storeName)) {
          const store = db.createObjectStore(this.storeName, { keyPath: 'url' });
          store.createIndex('version', 'version', { unique: false });
          store.createIndex('timestamp', 'timestamp', { unique: false });
        }
      };
    });
  }

  async getCachedModel(url: string, expectedVersion?: string): Promise<ArrayBuffer | null> {
    if (!this.db) await this.initDB();
    
    return new Promise((resolve, reject) => {
      const transaction = this.db!.transaction([this.storeName], 'readonly');
      const store = transaction.objectStore(this.storeName);
      const request = store.get(url);
      
      request.onerror = () => reject(request.error);
      request.onsuccess = () => {
        const result = request.result as ModelCache;
        
        if (!result) {
          resolve(null);
          return;
        }
        
        // ë²„ì „ í™•ì¸
        if (expectedVersion && result.version !== expectedVersion) {
          console.log(`ëª¨ë¸ ë²„ì „ ë¶ˆì¼ì¹˜: ìºì‹œ(${result.version}) vs ìš”ì²­(${expectedVersion})`);
          resolve(null);
          return;
        }
        
        // ìºì‹œ ë§Œë£Œ í™•ì¸ (ì˜ˆ: 24ì‹œê°„)
        const cacheExpiry = 24 * 60 * 60 * 1000; // 24ì‹œê°„
        if (Date.now() - result.timestamp > cacheExpiry) {
          console.log('ìºì‹œ ë§Œë£Œë¨');
          resolve(null);
          return;
        }
        
        resolve(result.data);
      };
    });
  }

  async cacheModel(url: string, data: ArrayBuffer, version: string): Promise<void> {
    if (!this.db) await this.initDB();
    
    const modelCache: ModelCache = {
      url,
      version,
      data,
      timestamp: Date.now()
    };
    
    return new Promise((resolve, reject) => {
      const transaction = this.db!.transaction([this.storeName], 'readwrite');
      const store = transaction.objectStore(this.storeName);
      const request = store.put(modelCache);
      
      request.onerror = () => reject(request.error);
      request.onsuccess = () => resolve();
    });
  }

  async loadModelWithCache(url: string, version?: string): Promise<ArrayBuffer> {
    // 1. ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
    const cachedData = await this.getCachedModel(url, version);
    if (cachedData) {
      console.log('ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©:', url);
      return cachedData;
    }
    
    // 2. ë„¤íŠ¸ì›Œí¬ì—ì„œ ë‹¤ìš´ë¡œë“œ
    console.log('ë„¤íŠ¸ì›Œí¬ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:', url);
    const response = await fetch(url);
    if (!response.ok) {
      throw new Error(`ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: ${response.status}`);
    }
    
    const data = await response.arrayBuffer();
    
    // 3. ìºì‹œì— ì €ì¥
    if (version) {
      await this.cacheModel(url, data, version);
    }
    
    return data;
  }

  async clearCache(): Promise<void> {
    if (!this.db) await this.initDB();
    
    return new Promise((resolve, reject) => {
      const transaction = this.db!.transaction([this.storeName], 'readwrite');
      const store = transaction.objectStore(this.storeName);
      const request = store.clear();
      
      request.onerror = () => reject(request.error);
      request.onsuccess = () => resolve();
    });
  }
}
```

### ëª¨ë¸ ë²„ì „ ê´€ë¦¬

```ts
interface ModelConfig {
  name: string;
  version: string;
  url: string;
  checksum?: string;
  size?: number;
}

class ModelVersionManager {
  private configUrl: string;
  private modelManager: ONNXModelManager;

  constructor(configUrl: string) {
    this.configUrl = configUrl;
    this.modelManager = new ONNXModelManager();
  }

  async getLatestModelConfig(modelName: string): Promise<ModelConfig> {
    const response = await fetch(this.configUrl);
    const configs: ModelConfig[] = await response.json();
    
    const modelConfig = configs.find(config => config.name === modelName);
    if (!modelConfig) {
      throw new Error(`ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${modelName}`);
    }
    
    return modelConfig;
  }

  async loadModel(modelName: string): Promise<ort.InferenceSession> {
    // 1. ìµœì‹  ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    const config = await this.getLatestModelConfig(modelName);
    
    // 2. ìºì‹œëœ ëª¨ë¸ ë˜ëŠ” ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ
    const modelData = await this.modelManager.loadModelWithCache(
      config.url, 
      config.version
    );
    
    // 3. ì²´í¬ì„¬ ê²€ì¦ (ì„ íƒì‚¬í•­)
    if (config.checksum) {
      const calculatedChecksum = await this.calculateChecksum(modelData);
      if (calculatedChecksum !== config.checksum) {
        throw new Error('ëª¨ë¸ ì²´í¬ì„¬ ë¶ˆì¼ì¹˜');
      }
    }
    
    // 4. ONNX ì„¸ì…˜ ìƒì„±
    const session = await ort.InferenceSession.create(modelData, {
      executionProviders: ["webgpu", "webgl", "wasm"],
    });
    
    return session;
  }

  private async calculateChecksum(data: ArrayBuffer): Promise<string> {
    const hashBuffer = await crypto.subtle.digest('SHA-256', data);
    const hashArray = Array.from(new Uint8Array(hashBuffer));
    return hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
  }
}
```

### ì‚¬ìš© ì˜ˆì œ

```ts
// ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
const modelManager = new ModelVersionManager('/api/models/config.json');

// ëª¨ë¸ ë¡œë“œ (ìºì‹± ë° ë²„ì „ ê´€ë¦¬ ìë™ ì²˜ë¦¬)
const session = await modelManager.loadModel('resnet50');

// ì¶”ë¡  ì‹¤í–‰
const results = await runInference(session, inputData, inputShape);
```

### ìºì‹œ ê´€ë¦¬ ì „ëµ

```ts
// ìºì‹œ í¬ê¸° ì œí•œ
class CacheManager extends ONNXModelManager {
  private maxCacheSize = 100 * 1024 * 1024; // 100MB

  async cleanupOldModels(): Promise<void> {
    if (!this.db) await this.initDB();
    
    const transaction = this.db!.transaction([this.storeName], 'readwrite');
    const store = transaction.objectStore(this.storeName);
    const index = store.index('timestamp');
    
    let totalSize = 0;
    const modelsToDelete: string[] = [];
    
    const request = index.openCursor(null, 'prev'); // ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
    
    request.onsuccess = (event) => {
      const cursor = (event.target as IDBRequest).result;
      if (cursor) {
        const model = cursor.value as ModelCache;
        totalSize += model.data.byteLength;
        
        if (totalSize > this.maxCacheSize) {
          modelsToDelete.push(model.url);
        }
        
        cursor.continue();
      } else {
        // ì˜¤ë˜ëœ ëª¨ë¸ë“¤ ì‚­ì œ
        modelsToDelete.forEach(url => {
          store.delete(url);
        });
      }
    };
  }
}
```

## ì‹¤ì œ Vision AI ëª¨ë¸ ì ìš© ì˜ˆì œ

### MobileNetì„ ì´ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜

```ts
interface ClassificationResult {
  label: string;
  probability: number;
}

class MobileNetClassifier {
  private session: ort.InferenceSession | null = null;
  private labels: string[] = [];

  async initialize() {
    // ImageNet í´ë˜ìŠ¤ ë¼ë²¨ ë¡œë“œ
    this.labels = await this.loadImageNetLabels();
    
    // MobileNet ëª¨ë¸ ë¡œë“œ
    this.session = await ort.InferenceSession.create('/models/mobilenet_v2.onnx', {
      executionProviders: ['webgpu', 'webgl', 'wasm']
    });
  }

  private async loadImageNetLabels(): Promise<string[]> {
    const response = await fetch('/labels/imagenet_labels.json');
    return await response.json();
  }

  async classifyImage(imageElement: HTMLImageElement): Promise<ClassificationResult[]> {
    if (!this.session) {
      throw new Error('ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }

    // 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (MobileNet ì…ë ¥ í˜•ì‹: 224x224, RGB, ì •ê·œí™”)
    const inputTensor = this.preprocessImageForMobileNet(imageElement);
    
    // 2. ì¶”ë¡  ì‹¤í–‰
    const feeds = { input: inputTensor };
    const results = await this.session.run(feeds);
    
    // 3. ê²°ê³¼ í›„ì²˜ë¦¬
    const output = results.output.data as Float32Array;
    const topResults = this.getTopPredictions(output, 5);
    
    return topResults;
  }

  private preprocessImageForMobileNet(imageElement: HTMLImageElement): ort.Tensor {
    const canvas = document.createElement('canvas');
    const ctx = canvas.getContext('2d')!;
    
    canvas.width = 224;
    canvas.height = 224;
    
    // ì´ë¯¸ì§€ë¥¼ 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
    ctx.drawImage(imageElement, 0, 0, 224, 224);
    
    const imageData = ctx.getImageData(0, 0, 224, 224);
    const pixels = imageData.data;
    
    // MobileNet ì „ì²˜ë¦¬: [0, 255] -> [-1, 1] ì •ê·œí™”
    const float32Data = new Float32Array(1 * 3 * 224 * 224);
    
    for (let i = 0; i < pixels.length; i += 4) {
      const pixelIndex = i / 4;
      
      // RGB ê°’ì„ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
      const r = (pixels[i] / 255.0 - 0.5) * 2;
      const g = (pixels[i + 1] / 255.0 - 0.5) * 2;
      const b = (pixels[i + 2] / 255.0 - 0.5) * 2;
      
      // NCHW í˜•ì‹ìœ¼ë¡œ ë°°ì¹˜
      float32Data[pixelIndex] = r;
      float32Data[pixelIndex + (224 * 224)] = g;
      float32Data[pixelIndex + (2 * 224 * 224)] = b;
    }
    
    return new ort.Tensor('float32', float32Data, [1, 3, 224, 224]);
  }

  private getTopPredictions(predictions: Float32Array, topK: number): ClassificationResult[] {
    const results: ClassificationResult[] = [];
    
    // ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©
    const softmaxResults = this.softmax(predictions);
    
    // í™•ë¥ ê³¼ ì¸ë±ìŠ¤ë¥¼ í•¨ê»˜ ì €ì¥
    const indexedResults = softmaxResults.map((prob, index) => ({
      probability: prob,
      index
    }));
    
    // í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬
    indexedResults.sort((a, b) => b.probability - a.probability);
    
    // ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜
    for (let i = 0; i < Math.min(topK, indexedResults.length); i++) {
      const result = indexedResults[i];
      results.push({
        label: this.labels[result.index] || `Class ${result.index}`,
        probability: result.probability
      });
    }
    
    return results;
  }

  private softmax(logits: Float32Array): Float32Array {
    const maxLogit = Math.max(...logits);
    const expLogits = logits.map(x => Math.exp(x - maxLogit));
    const sumExp = expLogits.reduce((sum, x) => sum + x, 0);
    
    return new Float32Array(expLogits.map(x => x / sumExp));
  }
}
```

### YOLOë¥¼ ì´ìš©í•œ ê°ì²´ íƒì§€

```ts
interface DetectionResult {
  label: string;
  confidence: number;
  bbox: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
}

class YOLODetector {
  private session: ort.InferenceSession | null = null;
  private labels: string[] = [];
  private inputSize = 640; // YOLOv5/v8 ê¸°ë³¸ ì…ë ¥ í¬ê¸°

  async initialize() {
    // COCO í´ë˜ìŠ¤ ë¼ë²¨ ë¡œë“œ
    this.labels = await this.loadCOCOLabels();
    
    // YOLO ëª¨ë¸ ë¡œë“œ
    this.session = await ort.InferenceSession.create('/models/yolov8n.onnx', {
      executionProviders: ['webgpu', 'webgl', 'wasm']
    });
  }

  private async loadCOCOLabels(): Promise<string[]> {
    const response = await fetch('/labels/coco_labels.json');
    return await response.json();
  }

  async detectObjects(imageElement: HTMLImageElement): Promise<DetectionResult[]> {
    if (!this.session) {
      throw new Error('ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }

    // 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    const { inputTensor, scaleX, scaleY } = this.preprocessImageForYOLO(imageElement);
    
    // 2. ì¶”ë¡  ì‹¤í–‰
    const feeds = { images: inputTensor };
    const results = await this.session.run(feeds);
    
    // 3. ê²°ê³¼ í›„ì²˜ë¦¬
    const output = results.output0.data as Float32Array;
    const detections = this.postprocessYOLOOutput(
      output, 
      scaleX, 
      scaleY, 
      imageElement.width, 
      imageElement.height
    );
    
    return detections;
  }

  private preprocessImageForYOLO(imageElement: HTMLImageElement) {
    const canvas = document.createElement('canvas');
    const ctx = canvas.getContext('2d')!;
    
    canvas.width = this.inputSize;
    canvas.height = this.inputSize;
    
    // ì›ë³¸ ì´ë¯¸ì§€ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ 640x640ì— ë§ì¶¤
    const scaleX = imageElement.width / this.inputSize;
    const scaleY = imageElement.height / this.inputSize;
    
    // ì´ë¯¸ì§€ë¥¼ ìº”ë²„ìŠ¤ ì¤‘ì•™ì— ë°°ì¹˜ (letterbox)
    ctx.fillStyle = '#808080'; // íšŒìƒ‰ íŒ¨ë”©
    ctx.fillRect(0, 0, this.inputSize, this.inputSize);
    
    const scale = Math.min(this.inputSize / imageElement.width, this.inputSize / imageElement.height);
    const scaledWidth = imageElement.width * scale;
    const scaledHeight = imageElement.height * scale;
    const offsetX = (this.inputSize - scaledWidth) / 2;
    const offsetY = (this.inputSize - scaledHeight) / 2;
    
    ctx.drawImage(imageElement, offsetX, offsetY, scaledWidth, scaledHeight);
    
    const imageData = ctx.getImageData(0, 0, this.inputSize, this.inputSize);
    const pixels = imageData.data;
    
    // YOLO ì „ì²˜ë¦¬: [0, 255] -> [0, 1] ì •ê·œí™”
    const float32Data = new Float32Array(1 * 3 * this.inputSize * this.inputSize);
    
    for (let i = 0; i < pixels.length; i += 4) {
      const pixelIndex = i / 4;
      
      const r = pixels[i] / 255.0;
      const g = pixels[i + 1] / 255.0;
      const b = pixels[i + 2] / 255.0;
      
      // NCHW í˜•ì‹ìœ¼ë¡œ ë°°ì¹˜
      float32Data[pixelIndex] = r;
      float32Data[pixelIndex + (this.inputSize * this.inputSize)] = g;
      float32Data[pixelIndex + (2 * this.inputSize * this.inputSize)] = b;
    }
    
    const inputTensor = new ort.Tensor('float32', float32Data, [1, 3, this.inputSize, this.inputSize]);
    
    return { inputTensor, scaleX, scaleY };
  }

  private postprocessYOLOOutput(
    output: Float32Array,
    scaleX: number,
    scaleY: number,
    originalWidth: number,
    originalHeight: number
  ): DetectionResult[] {
    const detections: DetectionResult[] = [];
    const confidenceThreshold = 0.5;
    const nmsThreshold = 0.4;
    
    // YOLOv8 ì¶œë ¥ í˜•ì‹: [1, 84, 8400] (84 = 4 bbox + 80 classes)
    const numDetections = 8400;
    const numClasses = 80;
    
    const boxes: number[][] = [];
    const scores: number[] = [];
    const classIds: number[] = [];
    
    for (let i = 0; i < numDetections; i++) {
      let maxScore = 0;
      let maxClassId = 0;
      
      // í´ë˜ìŠ¤ë³„ ìµœëŒ€ ì ìˆ˜ ì°¾ê¸°
      for (let j = 0; j < numClasses; j++) {
        const score = output[i + (4 + j) * numDetections];
        if (score > maxScore) {
          maxScore = score;
          maxClassId = j;
        }
      }
      
      if (maxScore > confidenceThreshold) {
        const cx = output[i]; // center x
        const cy = output[i + numDetections]; // center y
        const w = output[i + 2 * numDetections]; // width
        const h = output[i + 3 * numDetections]; // height
        
        // ì¤‘ì‹¬ì  ì¢Œí‘œë¥¼ ì¢Œìƒë‹¨ ì¢Œí‘œë¡œ ë³€í™˜
        const x = (cx - w / 2) * scaleX;
        const y = (cy - h / 2) * scaleY;
        const width = w * scaleX;
        const height = h * scaleY;
        
        boxes.push([x, y, width, height]);
        scores.push(maxScore);
        classIds.push(maxClassId);
      }
    }
    
    // NMS (Non-Maximum Suppression) ì ìš©
    const nmsIndices = this.applyNMS(boxes, scores, nmsThreshold);
    
    for (const index of nmsIndices) {
      const [x, y, width, height] = boxes[index];
      detections.push({
        label: this.labels[classIds[index]] || `Class ${classIds[index]}`,
        confidence: scores[index],
        bbox: {
          x: Math.max(0, Math.min(x, originalWidth)),
          y: Math.max(0, Math.min(y, originalHeight)),
          width: Math.min(width, originalWidth - x),
          height: Math.min(height, originalHeight - y)
        }
      });
    }
    
    return detections;
  }

  private applyNMS(boxes: number[][], scores: number[], threshold: number): number[] {
    const indices = Array.from({ length: boxes.length }, (_, i) => i);
    
    // ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    indices.sort((a, b) => scores[b] - scores[a]);
    
    const keep: number[] = [];
    
    while (indices.length > 0) {
      const current = indices.shift()!;
      keep.push(current);
      
      const remainingIndices: number[] = [];
      
      for (const index of indices) {
        const iou = this.calculateIoU(boxes[current], boxes[index]);
        if (iou <= threshold) {
          remainingIndices.push(index);
        }
      }
      
      indices.length = 0;
      indices.push(...remainingIndices);
    }
    
    return keep;
  }

  private calculateIoU(box1: number[], box2: number[]): number {
    const [x1, y1, w1, h1] = box1;
    const [x2, y2, w2, h2] = box2;
    
    const x1_max = x1 + w1;
    const y1_max = y1 + h1;
    const x2_max = x2 + w2;
    const y2_max = y2 + h2;
    
    const intersectionX = Math.max(0, Math.min(x1_max, x2_max) - Math.max(x1, x2));
    const intersectionY = Math.max(0, Math.min(y1_max, y2_max) - Math.max(y1, y2));
    const intersectionArea = intersectionX * intersectionY;
    
    const box1Area = w1 * h1;
    const box2Area = w2 * h2;
    const unionArea = box1Area + box2Area - intersectionArea;
    
    return intersectionArea / unionArea;
  }
}
```

### ì‚¬ìš© ì˜ˆì œ

```ts
// ì´ë¯¸ì§€ ë¶„ë¥˜ ì‚¬ìš© ì˜ˆì œ
const classifier = new MobileNetClassifier();
await classifier.initialize();

const imageElement = document.getElementById('input-image') as HTMLImageElement;
const classifications = await classifier.classifyImage(imageElement);

console.log('ë¶„ë¥˜ ê²°ê³¼:', classifications);
// ì¶œë ¥: [{ label: 'Egyptian cat', probability: 0.85 }, ...]

// ê°ì²´ íƒì§€ ì‚¬ìš© ì˜ˆì œ
const detector = new YOLODetector();
await detector.initialize();

const detections = await detector.detectObjects(imageElement);

console.log('íƒì§€ ê²°ê³¼:', detections);
// ì¶œë ¥: [{ label: 'person', confidence: 0.92, bbox: { x: 100, y: 50, width: 200, height: 300 } }, ...]

// íƒì§€ ê²°ê³¼ë¥¼ ìº”ë²„ìŠ¤ì— ì‹œê°í™”
function drawDetections(canvas: HTMLCanvasElement, detections: DetectionResult[]) {
  const ctx = canvas.getContext('2d')!;
  
  detections.forEach(detection => {
    const { bbox, label, confidence } = detection;
    
    // ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    ctx.strokeStyle = '#ff0000';
    ctx.lineWidth = 2;
    ctx.strokeRect(bbox.x, bbox.y, bbox.width, bbox.height);
    
    // ë¼ë²¨ ê·¸ë¦¬ê¸°
    ctx.fillStyle = '#ff0000';
    ctx.font = '16px Arial';
    const text = `${label} (${(confidence * 100).toFixed(1)}%)`;
    ctx.fillText(text, bbox.x, bbox.y - 5);
  });
}
```

## ì„±ëŠ¥ ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬

### ëª¨ë¸ ìµœì í™” ê¸°ë²•

```ts
class OptimizedONNXRunner {
  private session: ort.InferenceSession | null = null;
  private warmupCompleted = false;

  async initialize(modelUrl: string) {
    // 1. ìµœì í™”ëœ ì„¸ì…˜ ì˜µì…˜ ì„¤ì •
    const sessionOptions: ort.InferenceSession.SessionOptions = {
      executionProviders: ['webgpu', 'webgl', 'wasm'],
      graphOptimizationLevel: 'all', // ê·¸ë˜í”„ ìµœì í™” í™œì„±í™”
      enableCpuMemArena: true, // CPU ë©”ëª¨ë¦¬ ì•„ë ˆë‚˜ ì‚¬ìš©
      enableMemPattern: true, // ë©”ëª¨ë¦¬ íŒ¨í„´ ìµœì í™”
      executionMode: 'sequential', // ìˆœì°¨ ì‹¤í–‰ ëª¨ë“œ
      logId: 'onnx-session',
      logSeverityLevel: 2, // ê²½ê³  ìˆ˜ì¤€ ë¡œê·¸ë§Œ ì¶œë ¥
    };

    this.session = await ort.InferenceSession.create(modelUrl, sessionOptions);
    
    // 2. ì›œì—… ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    await this.warmup();
  }

  // ì›œì—…ì„ í†µí•œ ì´ˆê¸°í™” ìµœì í™”
  private async warmup() {
    if (!this.session || this.warmupCompleted) return;

    const inputNames = this.session.inputNames;
    const inputInfo = this.session.inputMetadata[inputNames[0]];
    
    // ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ JIT ì»´íŒŒì¼ ìµœì í™”
    const dummyInput = new ort.Tensor(
      'float32',
      new Float32Array(this.calculateTensorSize(inputInfo.dims as number[])),
      inputInfo.dims as number[]
    );

    const feeds = { [inputNames[0]]: dummyInput };
    
    // ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ ìµœì í™”
    for (let i = 0; i < 3; i++) {
      await this.session.run(feeds);
    }
    
    this.warmupCompleted = true;
    console.log('ì›œì—… ì™„ë£Œ');
  }

  private calculateTensorSize(dims: number[]): number {
    return dims.reduce((acc, dim) => acc * dim, 1);
  }
}
```

### ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ

```ts
class MemoryManager {
  private static instance: MemoryManager;
  private sessions: Map<string, ort.InferenceSession> = new Map();
  private tensors: Set<ort.Tensor> = new Set();
  private maxSessions = 3; // ìµœëŒ€ ì„¸ì…˜ ìˆ˜ ì œí•œ

  static getInstance(): MemoryManager {
    if (!MemoryManager.instance) {
      MemoryManager.instance = new MemoryManager();
    }
    return MemoryManager.instance;
  }

  async getSession(modelName: string, modelUrl: string): Promise<ort.InferenceSession> {
    // ê¸°ì¡´ ì„¸ì…˜ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
    if (this.sessions.has(modelName)) {
      return this.sessions.get(modelName)!;
    }

    // ì„¸ì…˜ ìˆ˜ ì œí•œ í™•ì¸
    if (this.sessions.size >= this.maxSessions) {
      await this.evictOldestSession();
    }

    // ìƒˆ ì„¸ì…˜ ìƒì„±
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu', 'webgl', 'wasm']
    });

    this.sessions.set(modelName, session);
    return session;
  }

  private async evictOldestSession() {
    const firstKey = this.sessions.keys().next().value;
    if (firstKey) {
      const session = this.sessions.get(firstKey)!;
      await this.releaseSession(firstKey);
    }
  }

  async releaseSession(modelName: string) {
    const session = this.sessions.get(modelName);
    if (session) {
      try {
        session.release();
        this.sessions.delete(modelName);
        console.log(`ì„¸ì…˜ í•´ì œë¨: ${modelName}`);
      } catch (error) {
        console.error(`ì„¸ì…˜ í•´ì œ ì‹¤íŒ¨: ${modelName}`, error);
      }
    }
  }

  // í…ì„œ ìƒì„± ì‹œ ì¶”ì 
  createTensor(type: string, data: Float32Array, dims: number[]): ort.Tensor {
    const tensor = new ort.Tensor(type as any, data, dims);
    this.tensors.add(tensor);
    return tensor;
  }

  // ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í…ì„œ í•´ì œ
  releaseTensor(tensor: ort.Tensor) {
    if (this.tensors.has(tensor)) {
      try {
        tensor.dispose();
        this.tensors.delete(tensor);
      } catch (error) {
        console.error('í…ì„œ í•´ì œ ì‹¤íŒ¨:', error);
      }
    }
  }

  // ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  async cleanup() {
    // ëª¨ë“  ì„¸ì…˜ í•´ì œ
    const sessionNames = Array.from(this.sessions.keys());
    for (const name of sessionNames) {
      await this.releaseSession(name);
    }

    // ëª¨ë“  í…ì„œ í•´ì œ
    this.tensors.forEach(tensor => {
      try {
        tensor.dispose();
      } catch (error) {
        console.error('í…ì„œ í•´ì œ ì‹¤íŒ¨:', error);
      }
    });
    this.tensors.clear();
  }

  // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
  getMemoryUsage() {
    return {
      activeSessions: this.sessions.size,
      activeTensors: this.tensors.size,
      jsHeapUsed: (performance as any).memory?.usedJSHeapSize || 0,
      jsHeapTotal: (performance as any).memory?.totalJSHeapSize || 0
    };
  }
}
```

### ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```ts
class BatchProcessor {
  private session: ort.InferenceSession;
  private batchSize: number;

  constructor(session: ort.InferenceSession, batchSize: number = 4) {
    this.session = session;
    this.batchSize = batchSize;
  }

  async processBatch(images: HTMLImageElement[]): Promise<any[]> {
    const results: any[] = [];
    
    // ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    for (let i = 0; i < images.length; i += this.batchSize) {
      const batch = images.slice(i, i + this.batchSize);
      const batchResults = await this.processSingleBatch(batch);
      results.push(...batchResults);
    }
    
    return results;
  }

  private async processSingleBatch(images: HTMLImageElement[]): Promise<any[]> {
    // ë°°ì¹˜ ì…ë ¥ í…ì„œ ìƒì„±
    const batchTensor = this.createBatchTensor(images);
    
    const feeds = { input: batchTensor };
    const outputs = await this.session.run(feeds);
    
    // ë°°ì¹˜ ì¶œë ¥ì„ ê°œë³„ ê²°ê³¼ë¡œ ë¶„ë¦¬
    return this.splitBatchOutput(outputs, images.length);
  }

  private createBatchTensor(images: HTMLImageElement[]): ort.Tensor {
    const batchSize = images.length;
    const [channels, height, width] = [3, 224, 224];
    
    const batchData = new Float32Array(batchSize * channels * height * width);
    
    images.forEach((image, batchIndex) => {
      const imageData = this.preprocessImage(image);
      const offset = batchIndex * channels * height * width;
      batchData.set(imageData, offset);
    });
    
    return new ort.Tensor('float32', batchData, [batchSize, channels, height, width]);
  }

  private preprocessImage(image: HTMLImageElement): Float32Array {
    // ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë¡œì§ (ì´ì „ ì˜ˆì œì™€ ë™ì¼)
    const canvas = document.createElement('canvas');
    const ctx = canvas.getContext('2d')!;
    
    canvas.width = 224;
    canvas.height = 224;
    ctx.drawImage(image, 0, 0, 224, 224);
    
    const imageData = ctx.getImageData(0, 0, 224, 224);
    const pixels = imageData.data;
    const float32Data = new Float32Array(3 * 224 * 224);
    
    for (let i = 0; i < pixels.length; i += 4) {
      const pixelIndex = i / 4;
      const r = (pixels[i] / 255.0 - 0.5) * 2;
      const g = (pixels[i + 1] / 255.0 - 0.5) * 2;
      const b = (pixels[i + 2] / 255.0 - 0.5) * 2;
      
      float32Data[pixelIndex] = r;
      float32Data[pixelIndex + (224 * 224)] = g;
      float32Data[pixelIndex + (2 * 224 * 224)] = b;
    }
    
    return float32Data;
  }

  private splitBatchOutput(outputs: any, batchSize: number): any[] {
    const results: any[] = [];
    const outputData = outputs[Object.keys(outputs)[0]].data;
    const outputSize = outputData.length / batchSize;
    
    for (let i = 0; i < batchSize; i++) {
      const start = i * outputSize;
      const end = start + outputSize;
      results.push(outputData.slice(start, end));
    }
    
    return results;
  }
}
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```ts
class PerformanceMonitor {
  private metrics: Map<string, number[]> = new Map();

  startTimer(label: string): string {
    const timerId = `${label}_${Date.now()}`;
    this.metrics.set(timerId, [performance.now()]);
    return timerId;
  }

  endTimer(timerId: string): number {
    const times = this.metrics.get(timerId);
    if (times) {
      const duration = performance.now() - times[0];
      times.push(duration);
      return duration;
    }
    return 0;
  }

  async measureInference(
    session: ort.InferenceSession,
    inputs: { [key: string]: ort.Tensor }
  ): Promise<{ results: any; metrics: any }> {
    const timerId = this.startTimer('inference');
    
    // GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (WebGPUì¸ ê²½ìš°)
    const beforeMemory = this.getMemoryUsage();
    
    const results = await session.run(inputs);
    
    const afterMemory = this.getMemoryUsage();
    const inferenceTime = this.endTimer(timerId);
    
    return {
      results,
      metrics: {
        inferenceTime,
        memoryDelta: afterMemory.jsHeapUsed - beforeMemory.jsHeapUsed,
        memoryUsage: afterMemory
      }
    };
  }

  private getMemoryUsage() {
    return {
      jsHeapUsed: (performance as any).memory?.usedJSHeapSize || 0,
      jsHeapTotal: (performance as any).memory?.totalJSHeapSize || 0,
      jsHeapLimit: (performance as any).memory?.jsHeapSizeLimit || 0
    };
  }

  getAverageTime(label: string): number {
    const allTimes: number[] = [];
    
    for (const [key, times] of this.metrics.entries()) {
      if (key.startsWith(label) && times.length > 1) {
        allTimes.push(times[1]); // ë‘ ë²ˆì§¸ ìš”ì†ŒëŠ” ì‹¤í–‰ ì‹œê°„
      }
    }
    
    return allTimes.length > 0 
      ? allTimes.reduce((sum, time) => sum + time, 0) / allTimes.length 
      : 0;
  }

  logPerformanceReport() {
    console.log('=== ì„±ëŠ¥ ë¦¬í¬íŠ¸ ===');
    console.log(`í‰ê·  ì¶”ë¡  ì‹œê°„: ${this.getAverageTime('inference').toFixed(2)}ms`);
    console.log('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:', this.getMemoryUsage());
    
    // ë¸Œë¼ìš°ì €ë³„ ì„±ëŠ¥ ì •ë³´
    if ('gpu' in navigator) {
      console.log('WebGPU ì§€ì›: ê°€ëŠ¥');
    } else {
      console.log('WebGPU ì§€ì›: ë¶ˆê°€ëŠ¥');
    }
  }
}
```

### ìµœì í™” ì ìš© ì˜ˆì œ

```ts
async function optimizedInference() {
  const memoryManager = MemoryManager.getInstance();
  const performanceMonitor = new PerformanceMonitor();
  
  try {
    // 1. ìµœì í™”ëœ ì„¸ì…˜ ë¡œë“œ
    const session = await memoryManager.getSession('mobilenet', '/models/mobilenet_v2.onnx');
    
    // 2. ë°°ì¹˜ í”„ë¡œì„¸ì„œ ìƒì„±
    const batchProcessor = new BatchProcessor(session, 4);
    
    // 3. ì´ë¯¸ì§€ ì¤€ë¹„
    const images = [
      document.getElementById('img1') as HTMLImageElement,
      document.getElementById('img2') as HTMLImageElement,
      document.getElementById('img3') as HTMLImageElement,
      document.getElementById('img4') as HTMLImageElement,
    ];
    
    // 4. ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    const timerId = performanceMonitor.startTimer('batch_inference');
    const results = await batchProcessor.processBatch(images);
    const processingTime = performanceMonitor.endTimer(timerId);
    
    console.log(`ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ${processingTime.toFixed(2)}ms`);
    console.log('ê²°ê³¼:', results);
    
    // 5. ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥
    performanceMonitor.logPerformanceReport();
    
  } catch (error) {
    console.error('ìµœì í™”ëœ ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨:', error);
  } finally {
    // 6. ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (í•„ìš”ì‹œ)
    // await memoryManager.cleanup();
  }
}

// í˜ì´ì§€ ì–¸ë¡œë“œ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
window.addEventListener('beforeunload', async () => {
  const memoryManager = MemoryManager.getInstance();
  await memoryManager.cleanup();
});
```

## ì—ëŸ¬ ì²˜ë¦¬ ë° ë””ë²„ê¹…

### ì¼ë°˜ì ì¸ ì—ëŸ¬ ìœ í˜•ê³¼ í•´ê²° ë°©ë²•

```ts
class ONNXErrorHandler {
  private static readonly ERROR_MESSAGES = {
    MODEL_LOAD_FAILED: 'ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨',
    INVALID_INPUT: 'ì˜ëª»ëœ ì…ë ¥ ë°ì´í„°',
    INFERENCE_FAILED: 'ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨',
    MEMORY_ERROR: 'ë©”ëª¨ë¦¬ ë¶€ì¡±',
    BACKEND_ERROR: 'ë°±ì—”ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨'
  };

  static handleError(error: any, context: string): never {
    console.error(`[${context}] ì—ëŸ¬ ë°œìƒ:`, error);
    
    if (error.message?.includes('no available backend found')) {
      throw new Error(`${this.ERROR_MESSAGES.BACKEND_ERROR}: ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. Vite ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.`);
    }
    
    if (error.message?.includes('Failed to fetch')) {
      throw new Error(`${this.ERROR_MESSAGES.MODEL_LOAD_FAILED}: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.`);
    }
    
    if (error.message?.includes('Input dimension mismatch')) {
      throw new Error(`${this.ERROR_MESSAGES.INVALID_INPUT}: ì…ë ¥ í…ì„œ í¬ê¸°ê°€ ëª¨ë¸ ìš”êµ¬ì‚¬í•­ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.`);
    }
    
    if (error.message?.includes('out of memory')) {
      throw new Error(`${this.ERROR_MESSAGES.MEMORY_ERROR}: GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ WASM ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.`);
    }
    
    throw new Error(`ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬: ${error.message}`);
  }
}
```

### ê°•ë ¥í•œ ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ê°€ì§„ ëª¨ë¸ ë˜í¼

```ts
class RobustONNXModel {
  private session: ort.InferenceSession | null = null;
  private retryCount = 0;
  private maxRetries = 3;
  private backoffDelay = 1000; // 1ì´ˆ

  async initialize(modelUrl: string): Promise<void> {
    for (let attempt = 0; attempt <= this.maxRetries; attempt++) {
      try {
        this.session = await this.createSessionWithFallback(modelUrl);
        console.log('ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ');
        return;
      } catch (error) {
        console.warn(`ì´ˆê¸°í™” ì‹œë„ ${attempt + 1}/${this.maxRetries + 1} ì‹¤íŒ¨:`, error);
        
        if (attempt < this.maxRetries) {
          await this.delay(this.backoffDelay * (attempt + 1));
        } else {
          ONNXErrorHandler.handleError(error, 'MODEL_INITIALIZATION');
        }
      }
    }
  }

  private async createSessionWithFallback(modelUrl: string): Promise<ort.InferenceSession> {
    const providers = ['webgpu', 'webgl', 'wasm'];
    let lastError: any;

    for (const provider of providers) {
      try {
        console.log(`${provider} ë°±ì—”ë“œë¡œ ì‹œë„ ì¤‘...`);
        const session = await ort.InferenceSession.create(modelUrl, {
          executionProviders: [provider],
        });
        console.log(`${provider} ë°±ì—”ë“œë¡œ ì„±ê³µ`);
        return session;
      } catch (error) {
        console.warn(`${provider} ë°±ì—”ë“œ ì‹¤íŒ¨:`, error);
        lastError = error;
        continue;
      }
    }

    throw lastError;
  }

  async predict(inputData: ort.Tensor): Promise<any> {
    if (!this.session) {
      throw new Error('ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }

    try {
      // ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬
      this.validateInput(inputData);
      
      // ì¶”ë¡  ì‹¤í–‰
      const feeds = { [this.session.inputNames[0]]: inputData };
      const results = await this.session.run(feeds);
      
      return results;
    } catch (error) {
      ONNXErrorHandler.handleError(error, 'INFERENCE');
    }
  }

  private validateInput(inputData: ort.Tensor): void {
    if (!this.session) return;

    const expectedInput = this.session.inputMetadata[this.session.inputNames[0]];
    const expectedShape = expectedInput.dims as number[];
    
    // ë™ì  ì°¨ì›(-1) ë¬´ì‹œí•˜ê³  ë¹„êµ
    const actualShape = inputData.dims;
    
    for (let i = 0; i < expectedShape.length; i++) {
      if (expectedShape[i] !== -1 && expectedShape[i] !== actualShape[i]) {
        throw new Error(
          `ì…ë ¥ í˜•íƒœ ë¶ˆì¼ì¹˜: ì˜ˆìƒ ${expectedShape}, ì‹¤ì œ ${actualShape}`
        );
      }
    }
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  async dispose(): Promise<void> {
    if (this.session) {
      try {
        this.session.release();
        this.session = null;
        console.log('ëª¨ë¸ ë¦¬ì†ŒìŠ¤ í•´ì œ ì™„ë£Œ');
      } catch (error) {
        console.error('ëª¨ë¸ í•´ì œ ì¤‘ ì—ëŸ¬:', error);
      }
    }
  }
}
```

### ë””ë²„ê¹… ë„êµ¬

```ts
class ONNXDebugger {
  private static logs: string[] = [];
  
  static enableDebugMode(): void {
    // ONNX Runtime ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    ort.env.logLevel = 'verbose';
    console.log('ONNX ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”');
  }

  static logModelInfo(session: ort.InferenceSession): void {
    console.group('=== ëª¨ë¸ ì •ë³´ ===');
    
    console.log('ì…ë ¥ ì •ë³´:');
    session.inputNames.forEach(name => {
      const metadata = session.inputMetadata[name];
      console.log(`  ${name}:`, {
        type: metadata.type,
        dims: metadata.dims
      });
    });
    
    console.log('ì¶œë ¥ ì •ë³´:');
    session.outputNames.forEach(name => {
      const metadata = session.outputMetadata[name];
      console.log(`  ${name}:`, {
        type: metadata.type,
        dims: metadata.dims
      });
    });
    
    console.groupEnd();
  }

  static logTensorInfo(tensor: ort.Tensor, name: string): void {
    const stats = this.calculateTensorStats(tensor.data as Float32Array);
    
    console.log(`í…ì„œ ì •ë³´ - ${name}:`, {
      type: tensor.type,
      dims: tensor.dims,
      size: tensor.size,
      stats
    });
  }

  private static calculateTensorStats(data: Float32Array) {
    const values = Array.from(data);
    const min = Math.min(...values);
    const max = Math.max(...values);
    const mean = values.reduce((sum, val) => sum + val, 0) / values.length;
    
    // í‘œì¤€í¸ì°¨ ê³„ì‚°
    const variance = values.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / values.length;
    const std = Math.sqrt(variance);
    
    return { min, max, mean: mean.toFixed(6), std: std.toFixed(6) };
  }

  static async profileInference(
    session: ort.InferenceSession,
    inputs: { [key: string]: ort.Tensor },
    runs: number = 10
  ): Promise<void> {
    console.log(`=== ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ (${runs}íšŒ ì‹¤í–‰) ===`);
    
    const times: number[] = [];
    
    // ì›œì—… ì‹¤í–‰
    await session.run(inputs);
    
    for (let i = 0; i < runs; i++) {
      const startTime = performance.now();
      await session.run(inputs);
      const endTime = performance.now();
      
      times.push(endTime - startTime);
    }
    
    const avgTime = times.reduce((sum, time) => sum + time, 0) / times.length;
    const minTime = Math.min(...times);
    const maxTime = Math.max(...times);
    
    console.log('í”„ë¡œíŒŒì¼ë§ ê²°ê³¼:', {
      í‰ê· ì‹œê°„: `${avgTime.toFixed(2)}ms`,
      ìµœì†Œì‹œê°„: `${minTime.toFixed(2)}ms`,
      ìµœëŒ€ì‹œê°„: `${maxTime.toFixed(2)}ms`,
      ì „ì²´ì‹œê°„: times
    });
  }

  static checkBrowserSupport(): void {
    console.group('=== ë¸Œë¼ìš°ì € ì§€ì› í˜„í™© ===');
    
    // WebGPU ì§€ì› í™•ì¸
    const webgpuSupported = 'gpu' in navigator;
    console.log('WebGPU ì§€ì›:', webgpuSupported ? 'âœ…' : 'âŒ');
    
    // WebGL ì§€ì› í™•ì¸
    const canvas = document.createElement('canvas');
    const webglContext = canvas.getContext('webgl') || canvas.getContext('experimental-webgl');
    console.log('WebGL ì§€ì›:', webglContext ? 'âœ…' : 'âŒ');
    
    // WASM ì§€ì› í™•ì¸
    const wasmSupported = typeof WebAssembly !== 'undefined';
    console.log('WebAssembly ì§€ì›:', wasmSupported ? 'âœ…' : 'âŒ');
    
    // SharedArrayBuffer ì§€ì› í™•ì¸ (ë©€í‹°ìŠ¤ë ˆë”©ìš©)
    const sharedArrayBufferSupported = typeof SharedArrayBuffer !== 'undefined';
    console.log('SharedArrayBuffer ì§€ì›:', sharedArrayBufferSupported ? 'âœ…' : 'âŒ');
    
    console.groupEnd();
  }

  static logError(error: any, context: string): void {
    const errorLog = `[${new Date().toISOString()}] ${context}: ${error.message}`;
    this.logs.push(errorLog);
    
    console.error(`ğŸš¨ ${context}:`, {
      message: error.message,
      stack: error.stack,
      name: error.name
    });
  }

  static exportLogs(): string {
    return this.logs.join('\n');
  }
}
```

### í†µí•© ë””ë²„ê¹… ì˜ˆì œ

```ts
async function debugONNXImplementation() {
  // 1. ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”
  ONNXDebugger.enableDebugMode();
  
  // 2. ë¸Œë¼ìš°ì € ì§€ì› í™•ì¸
  ONNXDebugger.checkBrowserSupport();
  
  try {
    // 3. ê°•ë ¥í•œ ëª¨ë¸ ì´ˆê¸°í™”
    const model = new RobustONNXModel();
    await model.initialize('/models/mobilenet_v2.onnx');
    
    // 4. ëª¨ë¸ ì •ë³´ ë¡œê·¸
    if (model['session']) {
      ONNXDebugger.logModelInfo(model['session']);
    }
    
    // 5. í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
    const testInput = new ort.Tensor(
      'float32',
      new Float32Array(1 * 3 * 224 * 224).fill(0.5),
      [1, 3, 224, 224]
    );
    
    // 6. ì…ë ¥ í…ì„œ ì •ë³´ ë¡œê·¸
    ONNXDebugger.logTensorInfo(testInput, 'í…ŒìŠ¤íŠ¸ ì…ë ¥');
    
    // 7. ì¶”ë¡  ì‹¤í–‰ ë° í”„ë¡œíŒŒì¼ë§
    const results = await model.predict(testInput);
    
    // 8. ì¶œë ¥ ê²°ê³¼ ë¡œê·¸
    Object.keys(results).forEach(key => {
      ONNXDebugger.logTensorInfo(results[key], `ì¶œë ¥: ${key}`);
    });
    
    // 9. ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
    if (model['session']) {
      await ONNXDebugger.profileInference(
        model['session'],
        { input: testInput },
        10
      );
    }
    
    console.log('âœ… ë””ë²„ê¹… ì™„ë£Œ');
    
  } catch (error) {
    ONNXDebugger.logError(error, 'DEBUG_IMPLEMENTATION');
    
    // ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥
    console.log('=== ì—ëŸ¬ ë¡œê·¸ ===');
    console.log(ONNXDebugger.exportLogs());
  }
}

// ì „ì—­ ì—ëŸ¬ í•¸ë“¤ëŸ¬ ì„¤ì¹˜
window.addEventListener('error', (event) => {
  ONNXDebugger.logError(event.error, 'GLOBAL_ERROR');
});

window.addEventListener('unhandledrejection', (event) => {
  ONNXDebugger.logError(event.reason, 'UNHANDLED_PROMISE_REJECTION');
});
```

### ì‹¤ìš©ì ì¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì²´í¬ë¦¬ìŠ¤íŠ¸

```ts
class TroubleshootingGuide {
  static async diagnose(): Promise<void> {
    console.log('ğŸ” ONNX Runtime Web ì§„ë‹¨ ì‹œì‘...\n');
    
    // 1. ê¸°ë³¸ í™˜ê²½ í™•ì¸
    this.checkEnvironment();
    
    // 2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
    await this.checkNetworkAccess();
    
    // 3. ëª¨ë¸ íŒŒì¼ í™•ì¸
    await this.checkModelAccess();
    
    // 4. Vite ì„¤ì • í™•ì¸
    this.checkViteConfig();
    
    console.log('âœ… ì§„ë‹¨ ì™„ë£Œ');
  }

  private static checkEnvironment(): void {
    console.log('ğŸ“‹ í™˜ê²½ í™•ì¸:');
    console.log('- User Agent:', navigator.userAgent);
    console.log('- HTTPS:', location.protocol === 'https:' ? 'âœ…' : 'âŒ');
    console.log('- localhost:', location.hostname === 'localhost' ? 'âœ…' : 'âŒ');
  }

  private static async checkNetworkAccess(): Promise<void> {
    console.log('\nğŸŒ ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼ í™•ì¸:');
    
    try {
      const response = await fetch(location.origin);
      console.log('- ì„œë²„ ì ‘ê·¼:', response.ok ? 'âœ…' : 'âŒ');
    } catch (error) {
      console.log('- ì„œë²„ ì ‘ê·¼: âŒ', error);
    }
  }

  private static async checkModelAccess(): Promise<void> {
    console.log('\nğŸ“¦ ëª¨ë¸ íŒŒì¼ ì ‘ê·¼ í™•ì¸:');
    
    const testPaths = ['/models/', '/public/models/', '/assets/models/'];
    
    for (const path of testPaths) {
      try {
        const response = await fetch(`${location.origin}${path}`);
        console.log(`- ${path}:`, response.ok ? 'âœ…' : 'âŒ');
      } catch (error) {
        console.log(`- ${path}: âŒ`);
      }
    }
  }

  private static checkViteConfig(): void {
    console.log('\nâš™ï¸ Vite ì„¤ì • í™•ì¸ ê°€ì´ë“œ:');
    console.log('ë‹¤ìŒ ì„¤ì •ì´ vite.config.tsì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:');
    console.log(`
export default defineConfig({
  assetsInclude: ["**/*.onnx"],
  optimizeDeps: {
    exclude: ["onnxruntime-web"],
  },
});
    `);
  }
}

// í˜ì´ì§€ ë¡œë“œ ì‹œ ìë™ ì§„ë‹¨ (ê°œë°œ ëª¨ë“œì—ì„œë§Œ)
if (process.env.NODE_ENV === 'development') {
  window.addEventListener('load', () => {
    TroubleshootingGuide.diagnose();
  });
}
```

## ë§ˆë¬´ë¦¬

ì´ ë¬¸ì„œì—ì„œëŠ” ONNX Runtime Webì„ ì‚¬ìš©í•˜ì—¬ ì›¹ í™˜ê²½ì—ì„œ AI ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ìì„¸íˆ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. Vite í™˜ê²½ì—ì„œì˜ ì„¤ì •ë¶€í„° ì‹¤ì œ Vision AI ëª¨ë¸ ì ìš©, ì„±ëŠ¥ ìµœì í™”, ì—ëŸ¬ ì²˜ë¦¬ê¹Œì§€ ì‹¤ë¬´ì—ì„œ í•„ìš”í•œ ëª¨ë“  ë‚´ìš©ì„ í¬í•¨í–ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” í¬ì¸íŠ¸ ìš”ì•½

1. **í™˜ê²½ ì„¤ì •**: Viteì—ì„œ `assetsInclude`ì™€ `optimizeDeps` ì„¤ì • í•„ìˆ˜
2. **ë°±ì—”ë“œ ì„ íƒ**: WebGPU > WebGL > WASM ìˆœì„œë¡œ í´ë°± ì „ëµ êµ¬í˜„
3. **ëª¨ë¸ ê´€ë¦¬**: IndexedDBë¥¼ í™œìš©í•œ ìºì‹±ê³¼ ë²„ì „ ê´€ë¦¬ë¡œ ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ
4. **ì„±ëŠ¥ ìµœì í™”**: ì›œì—…, ë°°ì¹˜ ì²˜ë¦¬, ë©”ëª¨ë¦¬ ê´€ë¦¬ë¡œ ìµœì  ì„±ëŠ¥ ë‹¬ì„±
5. **ì—ëŸ¬ ì²˜ë¦¬**: ê°•ë ¥í•œ ì—ëŸ¬ í•¸ë“¤ë§ê³¼ ë””ë²„ê¹… ë„êµ¬ë¡œ ì•ˆì •ì„± í™•ë³´

ì›¹ì—ì„œì˜ AI ëª¨ë¸ ì‹¤í–‰ì€ ì´ì œ ë” ì´ìƒ ì‹¤í—˜ì ì¸ ê¸°ìˆ ì´ ì•„ë‹™ë‹ˆë‹¤. ì ì ˆí•œ ìµœì í™”ì™€ ì—ëŸ¬ ì²˜ë¦¬ë¥¼ í†µí•´ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œë„ ì¶©ë¶„íˆ í™œìš©í•  ìˆ˜ ìˆëŠ” ì„±ìˆ™í•œ ê¸°ìˆ ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.
